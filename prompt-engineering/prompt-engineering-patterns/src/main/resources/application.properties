spring.application.name=prompt-engineering
spring.main.web-application-type=none

# 1. First you start by choosing a model. E.g. Anthropic Claude, OpenAI, etc.

# 2. Once you choose your model you will need to figure out the model configuration. 
# Most LLMs come with various configuration options that control the LLM’s output. 

# 2.1 select the provider's specifc model and set the connection and access options
spring.ai.anthropic.chat.options.model=claude-3-7-sonnet-latest
spring.ai.anthropic.api-key=${ANTHROPIC_API_KEY}
#spring.ai.openai.api-key=${OPENAI_API_KEY}

# 2.2 Output length controls the number of tokens to generate in a response.
spring.ai.anthropic.chat.options.max-tokens=500

# 2.3 Sampling controls the randomness of the model’s output.

# 2.3.1 Temperature
# Temperature controls the degree of randomness in token selection. 
# Lower temperatures are good for prompts that expect a more deterministic response, 
# while higher temperatures can lead to more diverse or unexpected results.
spring.ai.anthropic.chat.options.temperature=0.7

# 2.3.2 Top-K and top-P
# The best way to choose between top-K and top-P is to experiment with both methods
# (or both together) and see which one produces the results you are looking for.

# Top-K
# Selects the top K most likely tokens from the model’s predicted distribution. 
# The higher top-K, the more creative and varied the model’s output; 
# the lower top-K, the more restive and factual the model’s output. 
#spring.ai.anthropic.chat.options.top-k=

# Top-P
# Selects the top tokens whose cumulative probability does not exceed a certain value (P). 
# Values for P range from 0 (greedy decoding) to 1 (all tokens in the LLM’s vocabulary).
#spring.ai.anthropic.chat.options.top-p=1.0

# ...
#spring.ai.anthropic.chat.options.stop-sequences=



